{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6071a8b5-cdd0-4de7-a4a9-b76835ba1492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import file\n",
    "import pandas as pd\n",
    "df = pd.read_csv(r\"C:\\Users\\yugeshr\\Downloads\\JN\\economic times.csv\", delimiter=';')\n",
    "df.info()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b07541-5f7b-41f0-8d3b-20b0ef871e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase characters\n",
    "df['message']=df['message'].str.lower()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af7fc81-1bbd-455d-bee1-e007c5a69a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove whitespaces\n",
    "def remove_whitespaces(message):\n",
    "    if isinstance(message, str):\n",
    "        return ' '.join(message.split())\n",
    "    return message\n",
    "df['message']=df['message'].apply(remove_whitespaces)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317b5d46-fcc6-4cb6-9189-ee7c9501e8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize (creates a list of single entities e.g. words)\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "df['message'] = df['message'].astype(str)\n",
    "df['message']=df['message'].apply(lambda X: word_tokenize(X))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e18c5d-7e84-4bc1-bc32-3b4d8fec1c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove NLTK and custom stopwords\n",
    "\n",
    "# Load NLTK stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))\n",
    "en_stopwords = stopwords.words('english')\n",
    "\n",
    "\n",
    "# Remove the stopwords\n",
    "\n",
    "def remove_stopwords(message):\n",
    "    result = []\n",
    "    for token in message:\n",
    "        if token not in en_stopwords:\n",
    "            result.append(token)\n",
    "            \n",
    "    return result\n",
    "\n",
    "df['message'] = df['message'].apply(remove_stopwords)\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515d79e0-0a2d-41ac-bcfb-485e0613dd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def remove_punct(message):\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    lst=tokenizer.tokenize(' '.join(message))\n",
    "    return lst\n",
    "\n",
    "df['message'] = df['message'].apply(remove_punct)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2080c9e-3d5a-4b4a-b34e-cfa84a6177a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize,pos_tag\n",
    "\n",
    "def lemmatization(message):\n",
    "    \n",
    "    result=[]\n",
    "    wordnet = WordNetLemmatizer()\n",
    "    for token,tag in pos_tag(message):\n",
    "        pos=tag[0].lower()\n",
    "        \n",
    "        if pos not in ['a', 'r', 'n', 'v']:\n",
    "            pos='n'\n",
    "            \n",
    "        result.append(wordnet.lemmatize(token,pos))\n",
    "    \n",
    "    return result\n",
    "\n",
    "df['message']=df['message'].apply(lemmatization)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939228d1-8bb9-4145-9d24-bdf4836e72d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list to string\n",
    "df['message'] = [' '.join(map(str, l)) for l in df['message']]\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729d300b-0f48-4fd5-bf9d-cd271485b363",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove words 3 characters in length and less\n",
    "df['message'] = df.message.str.replace(r'\\b(\\w{1,2})\\b', '')\n",
    "# Remove whitespaces\n",
    "def remove_whitespaces(message):\n",
    "    return  \" \".join(message.split())\n",
    "df['message']=df['message'].apply(remove_whitespaces)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bcbac0-ebff-475d-bea1-5592e5e602a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import nltk\n",
    "nltk. download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "import pandas as pd\n",
    "\n",
    "#open the cleaned file \n",
    "df= pd.read_csv(r\"C:\\Users\\yugeshr\\Downloads\\JN\\economic timescleaned.csv\", delimiter=';')\n",
    "df['message'].fillna('', inplace=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335614f1-f1a0-4053-9614-e0821a2e9d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate sentiment\n",
    "analyzer = SIA()\n",
    "df['compound'] = [analyzer.polarity_scores(x)['compound'] for x in df['message']]\n",
    "df['negative'] = [analyzer.polarity_scores(x)['neg'] for x in df['message']]\n",
    "df['neutral'] = [analyzer.polarity_scores(x)['neu'] for x in df['message']]\n",
    "df['positive'] = [analyzer.polarity_scores(x)['pos'] for x in df['message']]\n",
    "\n",
    "#create columns\n",
    "df['sentiment']='neutral'\n",
    "df.loc[df.compound>0.05,'sentiment']='positive'\n",
    "df.loc[df.compound<-0.05,'sentiment']='negative'\n",
    "df.head()\n",
    "\n",
    "#on-screen summary\n",
    "print(df['sentiment'].value_counts()['neutral'])\n",
    "print(df['sentiment'].value_counts()['positive'])\n",
    "print(df['sentiment'].value_counts()['negative'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb53b074-b918-49de-9d7f-101c7dfe1b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to csv file\n",
    "df.to_csv(r\"C:\\Users\\yugeshr\\Downloads\\JN\\economic timessentiment.csv\", encoding='utf-8',index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
